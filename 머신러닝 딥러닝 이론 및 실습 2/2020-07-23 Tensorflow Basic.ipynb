{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-07-23 Tensorflow Basic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjAikzp_l7kb",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow 실습 1\n",
        "\n",
        "python의 딥러닝 라이브러리인 Tensorflow에 대해 알아보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2H2zFy4mBnZ",
        "colab_type": "text"
      },
      "source": [
        "## 기초 문법과 예제\n",
        "\n",
        "  Tensorflow는 graph로 연산을 나타내는 프로그래밍 시스템이다.\n",
        "\n",
        "  * Graph의 node는 연산, 즉 operation(op)을 수행\n",
        "  * Graph의 edge는 tensor의 흐름을 나타냄\n",
        "  * Authograph 및 eager execution을 통해, 'define by run'이 아닌 default로 제공 텐서(Tensor)\n",
        "  * Numpy의 ndarray와 비슷한 다차원 배열로, tensorflow에서의 데이터를 표현하는 구조\n",
        "  * 그래프 내 operation에서 tensor가 전달됨\n",
        "\n",
        "케라스 (Keras)\n",
        "\n",
        "  * 기존에 존재하는 딥러닝 프레임워크로, 쉽게 이해할 수 있는 코드로 구성되는 것이 특징\n",
        "  * Tensorflow에서 이를 적극적으로 활용하기로 결정해서 tf.keras라는 고수준 API로 다양한 layer를 사용할 수 있도록 함.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7oKM0T9mwqC",
        "colab_type": "text"
      },
      "source": [
        "## Tensorflow의 간단한 연산 예시\n",
        "\n",
        "* add, square, reduce_sum 등 간단한 함수들을 사용 가능\n",
        "* 연산의 결과는 tf.Tensor로 나오며, input 값의 형태에 따라 자동으로 dtype이 배정됨\n",
        "* 각각의 tf.Tensor는 shape와 dtype을 가지고 있음.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIDmjGzXmuma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "34fa4047-e0c7-4deb-dd44-03463f32ced8"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# constant tensor\n",
        "print(tf.constant([3, 7]))\n",
        "\n",
        "# 더하기\n",
        "print(tf.add(1, 2)) # scalar 값이면 shape가 안나옴\n",
        "\n",
        "# vector 더하기\n",
        "print(tf.add([1, 2], [3, 4]))\n",
        "\n",
        "# 제곱\n",
        "print(tf.square(5.0))\n",
        "\n",
        "# 합\n",
        "print(tf.reduce_sum([1, 2, 3]))\n",
        "\n",
        "# 각각 제곱 후 더하기 (연산자 오버로딩(overloading) 지원)\n",
        "print(tf.square(2) + tf.square(3))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([3 7], shape=(2,), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
            "tf.Tensor(25.0, shape=(), dtype=float32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(13, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YWRKfm2wM3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "d8d30407-629b-4fee-9bfb-041ef9b35117"
      },
      "source": [
        "# matrix 곱셈\n",
        "a = tf.constant([[2],[3]])\n",
        "b = tf.constant([[3,7]])\n",
        "x = tf.matmul(a,b)\n",
        "print(a.shape)\n",
        "print(b.shape)\n",
        "print(x)\n",
        "\n",
        "# tensor shape \n",
        "print(x.shape)\n",
        "\n",
        "# tensor data type\n",
        "print(x.dtype)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1)\n",
            "(1, 2)\n",
            "tf.Tensor(\n",
            "[[ 6 14]\n",
            " [ 9 21]], shape=(2, 2), dtype=int32)\n",
            "(2, 2)\n",
            "<dtype: 'int32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYvL60OCoIUV",
        "colab_type": "text"
      },
      "source": [
        "##tensorflow와 numpy의 호환성\n",
        "\n",
        "- list 또는 numpy array를 tf.Tensor로 변환할 때는, tf.convert_to_tensor 사용\n",
        "- tensorflow 연산은 numpy array를 자동으로 tf.Tensor로 변환하여 사용\n",
        "- 반대로, numpy 연산은 tf.Tensor를 numpy array로 변환함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK5fh-M1oK_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "349819f2-8846-49d4-8f42-3d5d935d0daa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "ndarray = np.ones([3, 3])\n",
        "# .convert_to_tensor 함수는 list, ndarray를 직접 텐서로 변환 \n",
        "print(tf.convert_to_tensor([[1,2],[3,4]], dtype=tf.float64)) #list를 바로 tensor로 변환\n",
        "print(tf.convert_to_tensor(ndarray, dtype=tf.int32)) # 위에서 정의한 ndarray를 tensor로 변환\n",
        "\n",
        "# 텐서플로 연산은 자동적으로 넘파이 배열을 텐서로 변환\n",
        "tensor = tf.multiply(ndarray, 2)\n",
        "print(tensor)\n",
        "\n",
        "# 반대로 넘파이 연산은 자동적으로 텐서를 넘파이 배열로 변환\n",
        "print(np.add(tensor, 1))\n",
        "\n",
        "# .numpy() 메서드는 텐서를 넘파이 배열로 변환\n",
        "print(tensor.numpy())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1. 2.]\n",
            " [3. 4.]], shape=(2, 2), dtype=float64)\n",
            "tf.Tensor(\n",
            "[[1 1 1]\n",
            " [1 1 1]\n",
            " [1 1 1]], shape=(3, 3), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]], shape=(3, 3), dtype=float64)\n",
            "[[3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]]\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbKKc2CJo115",
        "colab_type": "text"
      },
      "source": [
        "## Variable (변수)\n",
        "- 학습할 수 있는 parameter를 variable이라는 형태의 텐서로 표현함\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohAhmBWVoNNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f40678e4-cc3e-410c-c8cc-2c96cc0945f5"
      },
      "source": [
        "# tf.Vaiable()을 통해, input으로 list, numpy array, tensor 등을 넣어주면, variable로 변환\n",
        "my_variable = tf.Variable(tf.zeros([2, 3]))\n",
        "print(my_variable)\n",
        "\n",
        "v = tf.Variable([[2,3],[3,4]])\n",
        "print(v)\n",
        "\n",
        "# variable을 수식에 사용하면, 자동적으로 tf.Tensor로 변환되어 값을 표현\n",
        "w = v + 1\n",
        "print(w)\n",
        "print(v)\n",
        "\n",
        "# variable에 .read_value()를 이용할 경우, 현재 변수 값을 명시적으로 읽어올 수 있음\n",
        "print(v.read_value())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[0., 0., 0.],\n",
            "       [0., 0., 0.]], dtype=float32)>\n",
            "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
            "array([[2, 3],\n",
            "       [3, 4]], dtype=int32)>\n",
            "tf.Tensor(\n",
            "[[3 4]\n",
            " [4 5]], shape=(2, 2), dtype=int32)\n",
            "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
            "array([[2, 3],\n",
            "       [3, 4]], dtype=int32)>\n",
            "tf.Tensor(\n",
            "[[2 3]\n",
            " [3 4]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNuPEArHpWus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "ca666e1b-1742-4745-f205-c58afd58c8c2"
      },
      "source": [
        "# class에 tf.Module 등을 상속할 경우 .variables() 함수로 class가 보유한 변수 목록을 불러올 수 있음\n",
        "class MyModuleOne(tf.Module):\n",
        "    def __init__(self):\n",
        "        self.v0 = tf.Variable(1.0)\n",
        "        self.vs = [tf.Variable(x) for x in range(2)]\n",
        "\n",
        "m = MyModuleOne()\n",
        "print(m.variables)\n",
        "print(len(m.variables))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>)\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t7TxZhbpykE",
        "colab_type": "text"
      },
      "source": [
        "## 자동 미분 (gradient tape의 사용법)\n",
        "- gradient tape를 이용하면, variable에 대한 연산을 순서대로 모두 저장하고 자동으로 gradient를 계산하는 것이 가능\n",
        "- with 구문 안의 연산을 tape에 저장하면, tape.gradient(target, sources)로 우리가 원하는 형태의 미분 값을 계산할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69p7DLHTpzzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96bf2358-ffed-4270-d4f1-a38565c099cf"
      },
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "\n",
        "# dy/dy = 2x \n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJuUJ2HqBMT",
        "colab_type": "text"
      },
      "source": [
        "- 위의 미분 값은 scalar 형태이지만, gradient는 tensor형태도 될수 있음\n",
        "- sources 부분에 list를 넣으면 list가 출력되고, dictionary를 넣으면 dictionary 형태로 출력됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JtugqkvqCEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cfa43381-3a26-4581-fb62-2b9df29b4902"
      },
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b  ## 참고: tf.matmul(x, w) 와 x @ w 동일함\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2)\n",
            "(3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXylbrDDqb3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "bba2e727-1010-4941-c009-83e66f3a11c7"
      },
      "source": [
        "my_vars = {\n",
        "    'w': w,\n",
        "    'b': b\n",
        "}\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "print(grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'w': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
            "array([[ 4.6185956,  3.6943061],\n",
            "       [ 9.237191 ,  7.3886123],\n",
            "       [13.855787 , 11.082918 ]], dtype=float32)>, 'b': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.6185956, 3.6943061], dtype=float32)>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSVGybP6qve2",
        "colab_type": "text"
      },
      "source": [
        "## Device\n",
        "- tensorflow로 설치할 경우 CPU만 사용 가능\n",
        "- tensorflow-gpu로 설치할 경우 CPU, GPU 모두 사용 가능\n",
        "- with tf.device(\"디바이스 종류 및 번호\")\n",
        "위의 코드를 통해, 연산이 실행되는 device를 지정할 수 있음\n",
        "- 같은 연산일지라도, CPU보다는 GPU에서 훨씬 빠른 속도를 보임\n",
        "- Colab에서 GPU 설정하기: 런타임 -> 런타임 유형 변경 -> 하드웨어 가속기 GPU로 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PunQ4DAqnIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "565b9d81-845e-4ee0-9698-6c4314714ac0"
      },
      "source": [
        "import time\n",
        "\n",
        "def time_matmul(x):\n",
        "  start = time.time()\n",
        "  for loop in range(100):\n",
        "    tf.matmul(x, x)\n",
        "\n",
        "  result = time.time()-start\n",
        "  print(\"100 loops: {:0.2f}ms\".format(1000*result))\n",
        "\n",
        "# CPU에서 강제 실행합니다.\n",
        "print(tf.config.list_physical_devices('CPU'))\n",
        "if tf.config.list_physical_devices('CPU'):\n",
        "  print(\"On CPU:\")\n",
        "  with tf.device(\"CPU:0\"):\n",
        "    x = tf.random.uniform([1000, 1000])\n",
        "    assert x.device.endswith(\"CPU:0\")\n",
        "    time_matmul(x)\n",
        "\n",
        "# GPU #0가 이용가능시 GPU #0에서 강제 실행합니다.\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "  print(\"On GPU:\")\n",
        "  with tf.device(\"GPU:0\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\n",
        "    x = tf.random.uniform([1000, 1000])\n",
        "    assert x.device.endswith(\"GPU:0\")\n",
        "    time_matmul(x)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "On CPU:\n",
            "100 loops: 3481.35ms\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "On GPU:\n",
            "100 loops: 8.07ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meHjo9borEif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To-Do : 손으로 풀었던 문제를 코드로 구현\n",
        "import numpy as np\n",
        "\n",
        "# input data X와 정답 Y\n",
        "X = tf.Variable([[2], [3]], dtype=tf.float32)\n",
        "Y = tf.Variable([1], dtype=tf.float32)\n",
        "\n",
        "# 첫번째 layer W_1, 두번째 layer W_2\n",
        "W_1 = tf.Variable([[0.11, 0.21], [0.12, 0.08]])\n",
        "W_2 = tf.Variable([[0.14, 0.15]])\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o71xhatvPJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "7ecd4b30-c641-4e9a-89c9-f28462a9cf95"
      },
      "source": [
        "# forward 연산\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  H_1 = tf.matmul(W_1, X)\n",
        "  Y_hat = tf.matmul(W_2, H_1)\n",
        "  Loss = tf.square(Y_hat - Y) * 1/2\n",
        "  \n",
        "print(\"hidden nodes : \", H_1)\n",
        "print(\"Y_hat : \", Y_hat)\n",
        "print(\"Loss : \", Loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden nodes :  tf.Tensor(\n",
            "[[0.85]\n",
            " [0.48]], shape=(2, 1), dtype=float32)\n",
            "Y_hat :  tf.Tensor([[0.19100001]], shape=(1, 1), dtype=float32)\n",
            "Loss :  tf.Tensor([[0.32724053]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fThYiooiv5Jg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5fcc3a1d-aadc-4a6c-cf1d-1b548966cc18"
      },
      "source": [
        "# backward 연산 (모델 파라미터에 대해 gradient 계산)\n",
        "(dLoss_dW_1, dLoss_dW_2) = tape.gradient(Loss, [W_1, W_2])\n",
        "print(dLoss_dW_1)\n",
        "print(dLoss_dW_2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.22652    -0.33978   ]\n",
            " [-0.24270001 -0.36405003]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor([[-0.68765 -0.38832]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJkrdTc3wKdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "27124878-4971-47f5-e5ec-69c7e9af22f1"
      },
      "source": [
        "# learning rate 설정\n",
        "lr = 0.05\n",
        "\n",
        "# 모델 파라미터 업데이트\n",
        "W_1 = W_1 - lr * dLoss_dW_1\n",
        "W_2 = W_2 - lr * dLoss_dW_2\n",
        "\n",
        "# 업데이트 된 모델로 다시 forward 연산\n",
        "H_1 = tf.matmul(W_1, X)\n",
        "Y_hat = tf.matmul(W_2, H_1)\n",
        "Loss = tf.square(Y_hat - Y) * 1/2\n",
        "\n",
        "# Y_hat 값이 Y 값에 가까워지고, Loss 값이 작아진 것을 확인할 수 있음  \n",
        "print(\"hidden nodes : \", H_1)\n",
        "print(\"Y_hat : \", Y_hat)\n",
        "print(\"Loss : \", Loss)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden nodes :  tf.Tensor(\n",
            "[[0.923619  ]\n",
            " [0.55887747]], shape=(2, 1), dtype=float32)\n",
            "Y_hat :  tf.Tensor([[0.25574577]], shape=(1, 1), dtype=float32)\n",
            "Loss :  tf.Tensor([[0.27695718]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}